<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="We propose a physically realistic VLN platform and benchmark to bridge the embodied gap in Vision-and-Language Navigation.">
  <meta property="og:title" content="Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities"/>
  <meta property="og:description" content="We propose a physically realistic VLN platform and benchmark to bridge the embodied gap in Vision-and-Language Navigation."/>
  <meta property="og:url" content="https://vln-pe.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/fig1_overall.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities">
  <meta name="twitter:description" content="We propose a physically realistic VLN platform and benchmark to bridge the embodied gap in Vision-and-Language Navigation.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/fig1_overall.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision-and-Language Navigation, Physical Embodiment, Benchmark, Robot Navigation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon_48x48.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://crystalsixone.github.io/" target="_blank">Liuyi Wang</a><sup>1,2*</sup>,</span>
                <span class="author-block">
                  <a href="https://crystalsixone.github.io/" target="_blank">Xinyuan Xia</a><sup>2,3*</sup>,</span>
                  <span class="author-block">
                    <a target="_blank">Hui Zhao</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://hanqingwangai.github.io/" target="_blank">Hanqing Wang</a><sup>+2</sup>,</span>
                      <span class="author-block">
                        <a href="https://tai-wang.github.io/" target="_blank">Tai Wang</a><sup>2</sup>,</span>
                        <span class="author-block">
                          <a href="https://yilunchen.com/about/" target="_blank">Yilun Chen</a><sup>2</sup>,</span>
                          <span class="author-block">
                            <a href="https://ieeexplore.ieee.org/author/37677379800" target="_blank">Chengju Liu</a><sup>1</sup>,</span>
                            <span class="author-block">
                              <a href="https://ieeexplore.ieee.org/author/37276133600" target="_blank">Qijun Chen</a><sup>+1</sup>,</span>
                  <span class="author-block">
                    <a href="https://oceanpang.github.io/" target="_blank">Jiangmiao Pang</a><sup>2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Tongji University, </span>
                    <span class="author-block"><sup>2</sup>OpenRobotLab, Shanghai AI Laboratory, </span>
                    <span class="author-block"><sup>3</sup>Shanghai Jiao Tong University</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>+</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Video grid -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-multiline">
        <!-- Video 1 -->
        <div class="column is-6">
          <div class="video-container">
            <video poster="" id="video1" autoplay controls muted loop style="width: 100%;">
              <source src="static/videos/video_multiView_grayBg.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              ü§ñ Different viewpoints of the robots walking in VLN-PE.
            </h2>
          </div>
        </div>
        
        <!-- Video 2 -->
        <div class="column is-6">
          <div class="video-container">
            <video poster="" id="video2" autoplay controls muted loop style="width: 100%;">
              <source src="static/videos/video_grutopia_vln10_whiteBg.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              üó∫Ô∏è Trajectories and instructions from the GRU-VLN10 dataset.
            </h2>
          </div>
        </div>
        
        <!-- Video 3 -->
        <div class="column is-6">
          <div class="video-container">
            <video poster="" id="video3" autoplay controls muted loop style="width: 100%;">
              <source src="static/videos/video_3dgs_lab_whiteBg.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              üè¢ Trajectories and instructions from the 3DGS-Lab-VLN dataset.
            </h2>
          </div>
        </div>
        
        <!-- Video 4 -->
        <div class="column is-6">
          <div class="video-container">
            <video poster="" id="video4" autoplay controls muted loop style="width: 100%;">
              <source src="static/videos/video_light_whiteBg.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              üí° Comparisons of different lighting conditions.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video grid -->

<!-- Teaser Image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
      <div class="hero-body">
          <img src="static/images/fig1_overall.png" alt="An overview of our method"/>
          <h2 class="subtitle has-text-left">
            We introduce VLN-PE, a realistic VLN platform and benchmark designed to enhance physical deployment across diverse robot embodiments. It enables cross-embodiment data collection, evaluation, and optimization under realistic locomotion and environmental conditions. Through systematic experiments on ego-centric VLN methods, we expose critical physical and visual disparities that challenge existing approaches and benchmarks. VLN-PE offers a grounded framework to foster more generalizable VLN models for future physical embodied AI development.
          </h2>
      </div>
  </div>
</section>
<!-- End teaser Image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
      <div class="columns is-centered">
          <div class="column is-full">
              <div class="content">
                  <div class="level-set has-text-justified">
                      <p>
                        In this paper, we introduce VLN-PE, a physically realistic VLN platform and benchmark that provides a comprehensive environment for cross-embodiment (humanoid, quadruped, and wheeled) data collection and systematic evaluation of policies across various robot embodiments and environmental conditions.
                        Our experiments on VLN-PE reveal several critical insights that highlight limitations in current approaches and suggest promising directions for improvement:
                      <ol>
                          <li>
                            <em>SoTA Models Struggle in Physical Environments</em>: Existing VLN-CE models exhibit a 34% SR relative drop when transferred to physical settings, revealing a gap between pseudo-motion training and physical deployment.
                          </li>
                          <li>
                            <em>Cross-embodiment Sensitivity</em>: Model performance varies across different robots, primarily due to viewpoint height differences, highlighting the need for height-adaptive or perspective-invariant representations.
                          </li>
                          <li>
                            <em>Multi-Modal Robustness</em>: RGB-only models degrade significantly in low-light conditions, whereas RGB + depth models perform more reliably, underscoring the value of multi-modal fusion to improve the model's robustness.
                          </li>
                          <li>
                            <em>Limited Generalization of Standard Datasets</em>: MP3D-style datasets cannot fully capture environment shifts. A simple baseline with 6M trainable parameters, fine-tuned on our small-scale dataset of the newly introduced scenes, outperforms previous SoTA method in zero-shot settings, suggesting the importance of more diverse training distributions and comprehensive evaluation system.
                          </li>
                          <li>
                            <em>Towards Cross-Embodiment VLN</em>: In our experiments, co-training across different robots enables a single baseline to generalize across embodiments and achieve the SoTA result, showing an important foundation for the future unified cross-embodiment VLN model.
                          </li>
                      </ol>
                      </p>
                  </div>
              </div>
          </div>
      </div>
  </div>
  </div>
</section>


<!-- More Visualized Results -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
      <div class="columns is-centered">
          <div class="column is-full">
              <div class="content">
                  <div class="level-set has-text-justified">
                      <p>
                        Below, we provide some visualized results on the VLN-PE simulator.
                      </p>
                  </div>
              </div>

          </div>
      </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/fig7_h1_example.png" 
             alt="A figure showing the humanoid robot's navigation in the VLN-PE simulator."
             style="width: 80%; max-width: 800px; margin: auto; display: block;"/>
        <h2 class="subtitle has-text-left is-size-6">
          Visualization of the humanoid robot walking in VLN-PE. Leveraging the powerful interactive capabilities of Isaac Sim, researchers can easily observe robot motion from various perspectives within the environment.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/fig9_vlmaps.png" 
             alt="A figure showing the improved VLMaps in VLN-PE."
             style="width: 80%; max-width: 800px; margin: auto; display: block;"/>
        <h2 class="subtitle has-text-left is-size-6">
          Example of improved VLMaps. Blue dot: current position (black line: orientation). Green dot: frontiers (black line: exploration orientation). White: dilated obstacles. Light gray: explored area. Dark gray: unexplored area. Blue line: local planner trajectory.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/fig11_grutopia_vln10.png" 
           alt="A figure showing the GRU-VLN10 datasets."
           style="width: 80%; max-width: 800px; margin: auto; display: block;"/>
      <h2 class="subtitle has-text-centered is-size-6">
        Examples of trajectories and instructions from our introduced GRU-VLN10 datasets.
     </h2>
   </div>
   <div class="item">
    <!-- Your image here -->
    <img src="static/images/fig12_3dgs_lab.png" 
         alt="A figure showing the 3DGS-Lab-VLN datasets."
         style="width: 80%; max-width: 800px; margin: auto; display: block;"/>
    <h2 class="subtitle has-text-centered is-size-6">
      Examples of trajectories and instructions from our introduced 3DGS-Lab-VLN datasets.
   </h2>
 </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
      <div class="columns is-centered">
          <div class="column is-full">
              <div class="content">
                  <div class="level-set has-text-justified">
                      <p>
                        Based on the VLN-PE simulator, we conduct several experiments to evaluate the performance of different VLN models.
                      </p>
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/fig3_exp1.png" 
             alt="A table comparing the performance of different VLN models on VLN-PE in MP3D environments."
             style="width: 80%; max-width: 800px; margin: auto; display: block;"/>
        <h2 class="subtitle has-text-left is-size-6">
          Models transferred directly from VLN-CE to VLN-PE experience significant performance drops. Generally, the fine-tuned large video model NaVid exhibits better generalization compared to smaller models. Additionally, as a zero-shot, map-based LLM solution, VLMaps also demonstrates reasonable results in the VLN-PE setting.
          <br><br>
          Interestingly, fine-tuning the small models CMA (denoted as CMA+) with their SoTA weights (only 6M trainable parameters) using the training dataset from VLN-PE significantly improves performance. In particular, CMA+ (#11) surpasses NaVid's zero shot performance, achieving SR 28.72 vs. 21.58 and SPL 24.24 vs. 17.45 on val-seen.
          <br><br>
          These results indicate that existing VLN models tend to overfit to specific simulation platforms, resulting in poor generalization when directly transferred to new environments or settings. However, incorporating diverse domain data can further enhance overall navigation performance.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/fig_crossEmbodiment_effect.png" 
             alt="Two tables comparing the performance of models across different robot embodiments."
             style="width: 80%; max-width: 800px; margin: auto; display: block;"/>
        <h2 class="subtitle has-text-left is-size-6">
        The figure on the left evaluates NaVid's zero-shot performance on different robot types, revealing varying degrees of performance degradation. A key advantage of VLN-PE is its ability to seamlessly support diverse robot models for navigation and data collection. This raises our interest: <i>Can cross-robot data improve model training, enabling a unified model that generalizes across different robot embodiments?</i> To explore this, we collected R2R data from humanoid, quadrupedal, and wheeled robots within VLN-PE.
        <br><br>
        The figure on the right shows the cross-embodiment training significantly enhances overall performance and enables a "One-for-All" model. As indicated by the bolded results, models trained using a combination of data from all three robot types consistently achieve the best performance.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/fig_RDP.png" 
           alt="The structure of RDP."
           style="width: 40%; max-width: 400px; margin: auto; display: block;"/>
      <img src="static/images/fig4_exp2.png" 
           alt="The results of RDP on the GRU-VLN10 dataset."
           style="width: 80%; max-width: 800px; margin: auto; display: block;"/>
      <h2 class="subtitle has-text-left is-size-6">
        Furthermore, we introduce RDP, the first diffusion-based attempt in VLN, as a new baseline method, using the diffusion generative head to support multistep continuous prediction. On the out-of-mp3d-domain datasets GRU-VLN10 and 3DGS-Lab-VLN, the fine-tuned RDP achieves the SoTA results on both the seen and unseen environments.
     </h2>
   </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{wang2025rethinking,
        title={Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities},
        author={Wang, Liuyi and Xia, Xinyuan and Zhao, Hui and Wang, Hanqing and Wang, Tai and Chen, Yilun and Liu, Chengju and Chen, Qijun and Pang, Jiangmiao},
        journal={arXiv preprint arXiv:2503.14390},
        year={2025}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
